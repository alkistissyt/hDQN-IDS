# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1apN-g7PfVbAidYx_hBNDPxm0O4M5gc42
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile train.py

!pip install stable-baselines3[extra] gymnasium scikit-learn seaborn tqdm tensorboard



"""
Deep Q-Network Training Script for Hierarchical IDS
Trains both worker-only and hierarchical agents using Stable-Baselines3
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import psutil
import os
import pickle
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# Stable-Baselines3 imports
from stable_baselines3 import DQN
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv

# Import our custom modules
from data_preprocessing import CICIDSDataProcessor
from environment import EnvironmentFactory, WorkerOnlyEnvironment, HierarchicalIDSEnvironment, evaluate_environment

class MemoryTracker:
    """Track memory usage during training"""

    def __init__(self):
        self.memory_log = []

    def log_memory(self, stage: str):
        """Log current memory usage"""
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        self.memory_log.append({
            'stage': stage,
            'memory_mb': memory_mb
        })
        print(f"Memory usage at {stage}: {memory_mb:.2f} MB")

    def get_memory_summary(self) -> pd.DataFrame:
        """Get memory usage summary"""
        return pd.DataFrame(self.memory_log)

class MetricsCallback(BaseCallback):
    """Custom callback to track training metrics"""

    def __init__(self, eval_env, eval_freq: int = 10000, verbose: int = 1):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.metrics_history = []

    def _on_step(self) -> bool:
        if self.n_calls % self.eval_freq == 0:
            # Evaluate current model
            metrics = evaluate_environment(self.eval_env, self.model, num_episodes=5)
            metrics['timestep'] = self.n_calls
            self.metrics_history.append(metrics)

            if self.verbose > 0:
                print(f"Step {self.n_calls}: Accuracy={metrics['avg_accuracy']:.3f}, "
                      f"F1={metrics['avg_f1_score']:.3f}, Reward={metrics['avg_total_reward']:.2f}")

        return True

class DQNTrainer:
    """Main trainer class for DQN agents"""

    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize DQN trainer

        Args:
            config: Training configuration dictionary
        """
        self.config = config or self._get_default_config()
        self.memory_tracker = MemoryTracker()
        self.training_history = []

        # Models
        self.worker_model = None
        self.manager_model = None

        # Data
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None

    def _get_default_config(self) -> Dict:
        """Get default training configuration"""
        return {
            # Data parameters
            'pca_components': 20,
            'test_size': 0.2,
            'random_state': 42,

            # DQN parameters
            'learning_rate': 1e-4,
            'buffer_size': 100000,
            'learning_starts': 1000,
            'batch_size': 64,
            'tau': 1.0,
            'gamma': 0.99,
            'train_freq': 4,
            'gradient_steps': 1,
            'target_update_interval': 1000,

            # Epsilon-greedy parameters
            'exploration_fraction': 0.3,
            'exploration_initial_eps': 1.0,
            'exploration_final_eps': 0.05,

            # Training parameters
            'worker_timesteps': 500000,
            'manager_timesteps': 200000,
            'eval_freq': 10000,
            'save_freq': 50000,

            # Environment parameters
            'max_episodes': 1000,
            'episode_length': 100
        }

    def load_data(self, file_path: str):
        """
        Load and preprocess data

        Args:
            file_path: Path to the dataset CSV file
        """
        print("Loading and preprocessing data...")
        self.memory_tracker.log_memory("start_data_loading")

        # Initialize processor
        processor = CICIDSDataProcessor(pca_components=self.config['pca_components'])

        # Process dataset
        self.X_train, self.X_test, self.y_train, self.y_test = processor.process_dataset(
            file_path,
            test_size=self.config['test_size'],
            random_state=self.config['random_state']
        )

        # Save processor for later use
        processor.save_preprocessor('trained_preprocessor.pkl')

        self.memory_tracker.log_memory("data_loaded")

        print(f"Data loaded successfully!")
        print(f"Training samples: {len(self.X_train)}")
        print(f"Test samples: {len(self.X_test)}")
        print(f"Feature dimensions: {self.X_train.shape[1]}")
        print(f"Class distribution (train): {np.bincount(self.y_train)}")
        print(f"Class distribution (test): {np.bincount(self.y_test)}")

    def train_worker_agent(self) -> DQN:
        """
        Train the worker agent using WorkerOnlyEnvironment

        Returns:
            Trained worker DQN model
        """
        print("Training Worker Agent...")
        print("=" * 40)

        self.memory_tracker.log_memory("start_worker_training")

        # Create worker environment
        worker_env = EnvironmentFactory.create_worker_environment(
            self.X_train, self.y_train, max_episodes=self.config['max_episodes']
        )

        # Wrap environment for monitoring
        worker_env = Monitor(worker_env)
        worker_env = DummyVecEnv([lambda: worker_env])

        # Create evaluation environment
        eval_env = EnvironmentFactory.create_worker_environment(
            self.X_test, self.y_test, max_episodes=100
        )
        eval_env = Monitor(eval_env)

        # Create DQN model
        self.worker_model = DQN(
            'MlpPolicy',
            worker_env,
            learning_rate=self.config['learning_rate'],
            buffer_size=self.config['buffer_size'],
            learning_starts=self.config['learning_starts'],
            batch_size=self.config['batch_size'],
            tau=self.config['tau'],
            gamma=self.config['gamma'],
            train_freq=self.config['train_freq'],
            gradient_steps=self.config['gradient_steps'],
            target_update_interval=self.config['target_update_interval'],
            exploration_fraction=self.config['exploration_fraction'],
            exploration_initial_eps=self.config['exploration_initial_eps'],
            exploration_final_eps=self.config['exploration_final_eps'],
            verbose=1,
            tensorboard_log="./dqn_worker_tensorboard/"
        )

        # Create callback for metrics tracking
        metrics_callback = MetricsCallback(eval_env, eval_freq=self.config['eval_freq'])

        # Train the model
        print(f"Training worker for {self.config['worker_timesteps']} timesteps...")
        self.worker_model.learn(
            total_timesteps=self.config['worker_timesteps'],
            callback=[metrics_callback],
            progress_bar=True
        )

        # Save worker model
        self.worker_model.save("trained_worker_model")

        self.memory_tracker.log_memory("worker_training_complete")

        # Store training history
        self.training_history.append({
            'agent': 'worker',
            'metrics_history': metrics_callback.metrics_history
        })

        print("Worker agent training completed!")
        return self.worker_model

    def train_manager_agent(self) -> DQN:
        """
        Train the manager agent using HierarchicalIDSEnvironment

        Returns:
            Trained manager DQN model
        """
        if self.worker_model is None:
            raise ValueError("Worker model must be trained first!")

        print("Training Manager Agent...")
        print("=" * 40)

        self.memory_tracker.log_memory("start_manager_training")

        # Create hierarchical environment with trained worker
        manager_env = EnvironmentFactory.create_hierarchical_environment(
            self.X_train, self.y_train, self.worker_model, max_episodes=self.config['max_episodes']
        )

        # Wrap environment for monitoring
        manager_env = Monitor(manager_env)
        manager_env = DummyVecEnv([lambda: manager_env])

        # Create evaluation environment
        eval_env = EnvironmentFactory.create_hierarchical_environment(
            self.X_test, self.y_test, self.worker_model, max_episodes=100
        )
        eval_env = Monitor(eval_env)

        # Create DQN model for manager
        self.manager_model = DQN(
            'MlpPolicy',
            manager_env,
            learning_rate=self.config['learning_rate'],
            buffer_size=self.config['buffer_size'] // 2,  # Smaller buffer for manager
            learning_starts=self.config['learning_starts'] // 2,
            batch_size=self.config['batch_size'],
            tau=self.config['tau'],
            gamma=self.config['gamma'],
            train_freq=self.config['train_freq'],
            gradient_steps=self.config['gradient_steps'],
            target_update_interval=self.config['target_update_interval'],
            exploration_fraction=self.config['exploration_fraction'],
            exploration_initial_eps=self.config['exploration_initial_eps'],
            exploration_final_eps=self.config['exploration_final_eps'],
            verbose=1,
            tensorboard_log="./dqn_manager_tensorboard/"
        )

        # Create callback for metrics tracking
        metrics_callback = MetricsCallback(eval_env, eval_freq=self.config['eval_freq'])

        # Train the model
        print(f"Training manager for {self.config['manager_timesteps']} timesteps...")
        self.manager_model.learn(
            total_timesteps=self.config['manager_timesteps'],
            callback=[metrics_callback],
            progress_bar=True
        )

        # Save manager model
        self.manager_model.save("trained_manager_model")

        self.memory_tracker.log_memory("manager_training_complete")

        # Store training history
        self.training_history.append({
            'agent': 'manager',
            'metrics_history': metrics_callback.metrics_history
        })

        print("Manager agent training completed!")
        return self.manager_model

    def evaluate_models(self, num_episodes: int = 20) -> Dict:
        """
        Evaluate both worker and hierarchical models

        Args:
            num_episodes: Number of episodes for evaluation

        Returns:
            Dictionary containing evaluation results
        """
        print("Evaluating Models...")
        print("=" * 30)

        results = {}

        # Evaluate worker-only model
        if self.worker_model is not None:
            print("Evaluating Worker-Only Model...")
            worker_env = EnvironmentFactory.create_worker_environment(
                self.X_test, self.y_test, max_episodes=num_episodes
            )
            worker_results = evaluate_environment(worker_env, self.worker_model, num_episodes)
            results['worker_only'] = worker_results

            print(f"Worker-Only Results:")
            print(f"  Accuracy: {worker_results['avg_accuracy']:.4f} ± {worker_results['std_accuracy']:.4f}")
            print(f"  Precision: {worker_results['avg_precision']:.4f} ± {worker_results['std_precision']:.4f}")
            print(f"  Recall: {worker_results['avg_recall']:.4f} ± {worker_results['std_recall']:.4f}")
            print(f"  F1-Score: {worker_results['avg_f1_score']:.4f} ± {worker_results['std_f1_score']:.4f}")
            print(f"  Avg Reward: {worker_results['avg_total_reward']:.2f} ± {worker_results['std_total_reward']:.2f}")

        # Evaluate hierarchical model
        if self.manager_model is not None and self.worker_model is not None:
            print("\nEvaluating Hierarchical Model...")
            hierarchical_env = EnvironmentFactory.create_hierarchical_environment(
                self.X_test, self.y_test, self.worker_model, max_episodes=num_episodes
            )
            hierarchical_results = evaluate_environment(hierarchical_env, self.manager_model, num_episodes)
            results['hierarchical'] = hierarchical_results

            print(f"Hierarchical Results:")
            print(f"  Accuracy: {hierarchical_results['avg_accuracy']:.4f} ± {hierarchical_results['std_accuracy']:.4f}")
            print(f"  Precision: {hierarchical_results['avg_precision']:.4f} ± {hierarchical_results['std_precision']:.4f}")
            print(f"  Recall: {hierarchical_results['avg_recall']:.4f} ± {hierarchical_results['std_recall']:.4f}")
            print(f"  F1-Score: {hierarchical_results['avg_f1_score']:.4f} ± {hierarchical_results['std_f1_score']:.4f}")
            print(f"  Avg Reward: {hierarchical_results['avg_total_reward']:.2f} ± {hierarchical_results['std_total_reward']:.2f}")

        return results

    def generate_confusion_matrix(self, model, env_type: str = 'worker', num_episodes: int = 10):
        """
        Generate and plot confusion matrix

        Args:
            model: Trained model to evaluate
            env_type: Type of environment ('worker' or 'hierarchical')
            num_episodes: Number of episodes to run
        """
        print(f"Generating confusion matrix for {env_type} model...")

        # Create appropriate environment
        if env_type == 'worker':
            env = EnvironmentFactory.create_worker_environment(
                self.X_test, self.y_test, max_episodes=num_episodes
            )
        else:
            env = EnvironmentFactory.create_hierarchical_environment(
                self.X_test, self.y_test, self.worker_model, max_episodes=num_episodes
            )

        # Collect predictions
        y_true = []
        y_pred = []

        for episode in range(num_episodes):
            obs, info = env.reset()
            terminated = False
            step_count = 0

            while not terminated and step_count < 200:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)

                if 'true_label' in info:
                    y_true.append(info['true_label'])
                    # Convert action to binary prediction
                    if env_type == 'hierarchical':
                        # For hierarchical, we need to get the worker action
                        worker_action = info.get('worker_action', action)
                        y_pred.append(1 if worker_action == 1 else 0)  # 1 if block, 0 otherwise
                    else:
                        y_pred.append(1 if action == 1 else 0)  # 1 if block, 0 otherwise

                step_count += 1

        if len(y_true) == 0:
            print("No data collected for confusion matrix")
            return

        # Generate confusion matrix
        cm = confusion_matrix(y_true, y_pred)

        # Plot confusion matrix
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['Allow', 'Block'],
                   yticklabels=['Benign', 'Malicious'])
        plt.title(f'Confusion Matrix - {env_type.title()} Model')
        plt.xlabel('Predicted Action')
        plt.ylabel('True Label')
        plt.tight_layout()

        # Save plot
        plt.savefig(f'confusion_matrix_{env_type}.png', dpi=300, bbox_inches='tight')
        plt.show()

        # Print classification report
        print(f"\nClassification Report - {env_type.title()} Model:")
        print(classification_report(y_true, y_pred, target_names=['Benign', 'Malicious']))

    def plot_training_progress(self):
        """Plot training progress for both agents"""
        if not self.training_history:
            print("No training history available")
            return

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        for i, history in enumerate(self.training_history):
            agent_name = history['agent']
            metrics = history['metrics_history']

            if not metrics:
                continue

            # Extract data
            timesteps = [m['timestep'] for m in metrics]
            accuracy = [m['avg_accuracy'] for m in metrics]
            f1_score = [m['avg_f1_score'] for m in metrics]
            reward = [m['avg_total_reward'] for m in metrics]

            # Plot accuracy
            axes[0, 0].plot(timesteps, accuracy, label=f'{agent_name.title()} Agent', marker='o')
            axes[0, 0].set_title('Accuracy During Training')
            axes[0, 0].set_xlabel('Timesteps')
            axes[0, 0].set_ylabel('Accuracy')
            axes[0, 0].legend()
            axes[0, 0].grid(True)

            # Plot F1 score
            axes[0, 1].plot(timesteps, f1_score, label=f'{agent_name.title()} Agent', marker='s')
            axes[0, 1].set_title('F1-Score During Training')
            axes[0, 1].set_xlabel('Timesteps')
            axes[0, 1].set_ylabel('F1-Score')
            axes[0, 1].legend()
            axes[0, 1].grid(True)

            # Plot reward
            axes[1, 0].plot(timesteps, reward, label=f'{agent_name.title()} Agent', marker='^')
            axes[1, 0].set_title('Average Reward During Training')
            axes[1, 0].set_xlabel('Timesteps')
            axes[1, 0].set_ylabel('Average Reward')
            axes[1, 0].legend()
            axes[1, 0].grid(True)

        # Memory usage plot
        memory_df = self.memory_tracker.get_memory_summary()
        if not memory_df.empty:
            axes[1, 1].plot(range(len(memory_df)), memory_df['memory_mb'], marker='d')
            axes[1, 1].set_title('Memory Usage During Training')
            axes[1, 1].set_xlabel('Training Stage')
            axes[1, 1].set_ylabel('Memory Usage (MB)')
            axes[1, 1].set_xticks(range(len(memory_df)))
            axes[1, 1].set_xticklabels(memory_df['stage'], rotation=45)
            axes[1, 1].grid(True)

        plt.tight_layout()
        plt.savefig('training_progress.png', dpi=300, bbox_inches='tight')
        plt.show()

    def save_results(self, filename: str = 'training_results.pkl'):
        """Save training results and models"""
        results = {
            'config': self.config,
            'training_history': self.training_history,
            'memory_usage': self.memory_tracker.get_memory_summary().to_dict(),
            'data_shapes': {
                'X_train': self.X_train.shape if self.X_train is not None else None,
                'X_test': self.X_test.shape if self.X_test is not None else None,
                'y_train': self.y_train.shape if self.y_train is not None else None,
                'y_test': self.y_test.shape if self.y_test is not None else None,
            }
        }

        with open(filename, 'wb') as f:
            pickle.dump(results, f)

        print(f"Results saved to {filename}")

def main():
    """
    Main training function
    Usage example for Google Colab
    """
    print("Hierarchical DQN Training for Intrusion Detection")
    print("=" * 50)

    # Configuration
    config = {
      # Dataset + PCA
          'pca_components': 20,
          'test_size': 0.2, #added after
          'random_state': 42, #added after

        # Worker & manager training steps
          'worker_timesteps': 200000, # Reduced for Colab
          'manager_timesteps': 100000, # Reduced for Colab

        # DQN hyperparameters
         'learning_rate': 1e-4,
          'buffer_size': 50000,
          'learning_starts': 1000,
          'batch_size': 32,
          'tau': 1.0,
          'gamma': 0.99,
          'train_freq': 4,
          'gradient_steps': 1,
          'target_update_interval': 1000,

         # Epsilon-greedy exploration #added after
          'exploration_fraction': 0.3,
          'exploration_initial_eps': 1.0,
          'exploration_final_eps': 0.05,

        # Evaluation & environment #added after
          'eval_freq': 5000,
          'max_episodes': 1000
    }

    # Initialize trainer
    trainer = DQNTrainer(config)

    # File path - FIXED VERSION with proper escaping
    # Use one of these approaches:

    # Option 1: Raw string (recommended)
    #file_path = r"C:\Users\User\Downloads\IDS_RL\RawData\MachineLearningCSV\MachineLearningCVE\Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv"

    # Option 2: Forward slashes
    # file_path = "C:/Users/User/Downloads/IDS_RL/RawData/MachineLearningCSV/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv"

    # Option 3: Escaped backslashes
    # file_path = "C:\\Users\\User\\Downloads\\IDS_RL\\RawData\\MachineLearningCSV\\MachineLearningCVE\\Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv"

    try:
        # Load and preprocess data
        trainer.load_data(file_path)

        # Train worker agent
        worker_model = trainer.train_worker_agent()

        # Train manager agent
        manager_model = trainer.train_manager_agent()

        # Evaluate models
        results = trainer.evaluate_models(num_episodes=10)

        # Generate confusion matrices
        trainer.generate_confusion_matrix(worker_model, 'worker', num_episodes=5)
        trainer.generate_confusion_matrix(manager_model, 'hierarchical', num_episodes=5)

        # Plot training progress
        trainer.plot_training_progress()

        # Save results
        trainer.save_results()

        print("\nTraining completed successfully!")
        print("Files saved:")
        print("- trained_worker_model.zip: Trained worker DQN model")
        print("- trained_manager_model.zip: Trained manager DQN model")
        print("- trained_preprocessor.pkl: Data preprocessor")
        print("- training_results.pkl: Complete training results")
        print("- confusion_matrix_*.png: Confusion matrices")
        print("- training_progress.png: Training progress plots")

    except FileNotFoundError:
        print(f"Error: File {file_path} not found.")
        print("Please check the file path and ensure the dataset is available.")
        print("\nFor Google Colab, you can upload files using:")
        print("from google.colab import files")
        print("uploaded = files.upload()")
        print("file_path = list(uploaded.keys())[0]")
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        print("Please check the error message and try again.")

if __name__ == "__main__":
    main()

from google.colab import files
uploaded = files.upload()
file_path = list(uploaded.keys())[0]

from google.colab import files
uploaded = files.upload()
file_path = list(uploaded.keys())[0]

from google.colab import files
uploaded = files.upload()
file_path = list(uploaded.keys())[0]
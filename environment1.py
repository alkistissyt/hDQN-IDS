# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12modho26S1Xky7USjwZLjdJ1hrlibuTg
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile environment.py

"""
Hierarchical Intrusion Detection System Environment
Implements Gym-compatible environments for training DQN agents
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
from typing import Tuple, Dict, Any, Optional
import random
from collections import deque

class BaseIDSEnvironment(gym.Env):
    """Base class for IDS environments"""

    def __init__(self, X_data: np.ndarray, y_data: np.ndarray, max_episodes: int = 1000):
        """
        Initialize base IDS environment

        Args:
            X_data: Feature data
            y_data: Labels (0=benign, 1=malicious)
            max_episodes: Maximum number of episodes
        """
        super().__init__()

        self.X_data = X_data
        self.y_data = y_data
        self.max_episodes = max_episodes
        self.data_size = len(X_data)

        # Episode tracking
        self.current_episode = 0
        self.current_step = 0
        self.episode_length = min(100, self.data_size // 10)  # Adaptive episode length

        # Data indexing
        self.data_indices = np.arange(self.data_size)
        self.current_data_idx = 0

        # Metrics tracking
        self.episode_metrics = {
            'true_positives': 0,
            'false_positives': 0,
            'true_negatives': 0,
            'false_negatives': 0,
            'total_reward': 0,
            'lightweight_mode_usage': 0,
            'full_mode_usage': 0
        }

        # History for observation
        self.observation_history = deque(maxlen=5)

    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:
        """Reset the environment"""
        super().reset(seed=seed)

        # Shuffle data indices for each episode
        np.random.shuffle(self.data_indices)
        self.current_data_idx = 0
        self.current_step = 0

        # Reset metrics
        self.episode_metrics = {
            'true_positives': 0,
            'false_positives': 0,
            'true_negatives': 0,
            'false_negatives': 0,
            'total_reward': 0,
            'lightweight_mode_usage': 0,
            'full_mode_usage': 0
        }

        # Clear history
        self.observation_history.clear()

        # Get initial observation
        observation = self._get_observation()
        info = self._get_info()

        return observation, info

    def _get_current_sample(self) -> Tuple[np.ndarray, int]:
        """Get current data sample"""
        if self.current_data_idx >= self.data_size:
            self.current_data_idx = 0
            np.random.shuffle(self.data_indices)

        idx = self.data_indices[self.current_data_idx]
        return self.X_data[idx], self.y_data[idx]

    def _get_observation(self) -> np.ndarray:
        """Get current observation - to be implemented by subclasses"""
        raise NotImplementedError

    def _get_info(self) -> Dict:
        """Get info dictionary"""
        current_sample, true_label = self._get_current_sample()
        return {
            'current_step': self.current_step,
            'episode_length': self.episode_length,
            'true_label': true_label,
            'metrics': self.episode_metrics.copy()
        }

    def _update_metrics(self, action: int, true_label: int, reward: float):
        """Update episode metrics"""
        self.episode_metrics['total_reward'] += reward

        # For binary classification with actions: 0=allow, 1=block, 2=log
        if action == 1:  # Block
            if true_label == 1:  # Correctly blocked malicious
                self.episode_metrics['true_positives'] += 1
            else:  # Incorrectly blocked benign
                self.episode_metrics['false_positives'] += 1
        elif action == 0:  # Allow
            if true_label == 0:  # Correctly allowed benign
                self.episode_metrics['true_negatives'] += 1
            else:  # Incorrectly allowed malicious
                self.episode_metrics['false_negatives'] += 1
        # Action 2 (log) doesn't affect classification metrics directly

class WorkerOnlyEnvironment(BaseIDSEnvironment):
    """
    Worker-only environment for initial training
    Worker agent chooses: 0=allow, 1=block, 2=log
    """

    def __init__(self, X_data: np.ndarray, y_data: np.ndarray, max_episodes: int = 1000):
        super().__init__(X_data, y_data, max_episodes)

        # Action space: 0=allow, 1=block, 2=log
        self.action_space = spaces.Discrete(3)

        # Observation space: current sample + basic statistics
        feature_dim = X_data.shape[1]
        obs_dim = feature_dim + 10  # features + statistics
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32
        )

        # Reward parameters
        self.reward_block_malicious = 10.0      # Reward for blocking attack
        self.penalty_allow_malicious = -20.0    # Penalty for allowing attack
        self.penalty_block_benign = -2.0        # Penalty for false positive
        self.reward_allow_benign = 1.0          # Reward for allowing benign
        self.reward_log = 0.5                   # Small reward for logging

    def _get_observation(self) -> np.ndarray:
        """Get observation including current sample and statistics"""
        current_sample, _ = self._get_current_sample()

        # Basic statistics from recent history
        stats = np.array([
            self.current_step / self.episode_length,  # Progress
            self.episode_metrics['true_positives'],
            self.episode_metrics['false_positives'],
            self.episode_metrics['true_negatives'],
            self.episode_metrics['false_negatives'],
            self.episode_metrics['total_reward'] / max(1, self.current_step),  # Avg reward
            len(self.observation_history) / 5.0,  # History fullness
            np.mean([obs[0] for obs in self.observation_history]) if self.observation_history else 0,
            np.std([obs[0] for obs in self.observation_history]) if len(self.observation_history) > 1 else 0,
            random.random()  # Add some randomness
        ], dtype=np.float32)

        # Combine current sample with statistics
        observation = np.concatenate([current_sample, stats])

        # Store in history
        self.observation_history.append((np.mean(current_sample), len(current_sample)))

        return observation

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """Execute one environment step"""
        current_sample, true_label = self._get_current_sample()

        # Calculate reward
        reward = self._calculate_reward(action, true_label)

        # Update metrics
        self._update_metrics(action, true_label, reward)

        # Move to next step
        self.current_step += 1
        self.current_data_idx += 1

        # Check if episode is done
        terminated = self.current_step >= self.episode_length
        truncated = False

        # Get next observation
        if not terminated:
            observation = self._get_observation()
        else:
            observation = np.zeros(self.observation_space.shape, dtype=np.float32)

        info = self._get_info()

        return observation, reward, terminated, truncated, info

    def _calculate_reward(self, action: int, true_label: int) -> float:
        """Calculate reward based on action and true label"""
        if action == 1:  # Block
            if true_label == 1:  # Correctly blocked malicious
                return self.reward_block_malicious
            else:  # Incorrectly blocked benign (false positive)
                return self.penalty_block_benign
        elif action == 0:  # Allow
            if true_label == 0:  # Correctly allowed benign
                return self.reward_allow_benign
            else:  # Incorrectly allowed malicious (false negative)
                return self.penalty_allow_malicious
        else:  # Log (action == 2)
            return self.reward_log

class HierarchicalIDSEnvironment(BaseIDSEnvironment):
    """
    Hierarchical environment with manager and worker agents
    Manager chooses detection mode: 0=lightweight, 1=full
    Worker chooses action: 0=allow, 1=block, 2=log
    """

    def __init__(self, X_data: np.ndarray, y_data: np.ndarray, worker_agent=None, max_episodes: int = 1000):
        super().__init__(X_data, y_data, max_episodes)

        self.worker_agent = worker_agent

        # Action space for manager: 0=lightweight, 1=full
        self.action_space = spaces.Discrete(2)

        # Observation space: current sample + extended statistics
        feature_dim = X_data.shape[1]
        obs_dim = feature_dim + 15  # features + extended statistics
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32
        )

        # Cost parameters
        self.cost_lightweight = 0.1
        self.cost_full = 1.0

        # Mode tracking
        self.current_mode = 0

    def _get_observation(self) -> np.ndarray:
        """Get observation for manager agent"""
        current_sample, _ = self._get_current_sample()

        # Extended statistics for manager decision
        stats = np.array([
            self.current_step / self.episode_length,  # Progress
            self.episode_metrics['true_positives'],
            self.episode_metrics['false_positives'],
            self.episode_metrics['true_negatives'],
            self.episode_metrics['false_negatives'],
            self.episode_metrics['total_reward'] / max(1, self.current_step),  # Avg reward
            self.episode_metrics['lightweight_mode_usage'] / max(1, self.current_step),
            self.episode_metrics['full_mode_usage'] / max(1, self.current_step),
            # Sample complexity indicators
            np.mean(current_sample),
            np.std(current_sample),
            np.max(current_sample),
            np.min(current_sample),
            # Recent performance
            len(self.observation_history) / 5.0,
            np.mean([obs[0] for obs in self.observation_history]) if self.observation_history else 0,
            random.random()  # Randomness
        ], dtype=np.float32)

        # Combine current sample with statistics
        observation = np.concatenate([current_sample, stats])

        # Store in history
        self.observation_history.append((np.mean(current_sample), np.std(current_sample)))

        return observation

    def step(self, manager_action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """Execute hierarchical environment step"""
        if self.worker_agent is None:
            raise ValueError("Worker agent must be provided for hierarchical environment")

        current_sample, true_label = self._get_current_sample()
        self.current_mode = manager_action

        # Update mode usage
        if manager_action == 0:
            self.episode_metrics['lightweight_mode_usage'] += 1
        else:
            self.episode_metrics['full_mode_usage'] += 1

        # Get worker observation (simulated based on detection mode)
        worker_obs = self._get_worker_observation(current_sample, manager_action)

        # Get worker action
        worker_action, _ = self.worker_agent.predict(worker_obs, deterministic=False)

        # Calculate reward
        reward = self._calculate_hierarchical_reward(manager_action, worker_action, true_label)

        # Update metrics
        self._update_metrics(worker_action, true_label, reward)

        # Move to next step
        self.current_step += 1
        self.current_data_idx += 1

        # Check if episode is done
        terminated = self.current_step >= self.episode_length
        truncated = False

        # Get next observation
        if not terminated:
            observation = self._get_observation()
        else:
            observation = np.zeros(self.observation_space.shape, dtype=np.float32)

        info = self._get_info()
        info['manager_action'] = manager_action
        info['worker_action'] = worker_action
        info['detection_mode'] = 'lightweight' if manager_action == 0 else 'full'

        return observation, reward, terminated, truncated, info

    def _get_worker_observation(self, sample: np.ndarray, mode: int) -> np.ndarray:
        """
        Get worker observation based on detection mode
        Lightweight mode: reduced feature set
        Full mode: complete feature set
        """
        if mode == 0:  # Lightweight mode - use subset of features
            # Use first 60% of features for lightweight mode
            feature_subset_size = int(0.6 * len(sample))
            reduced_sample = sample[:feature_subset_size]

            # Pad to match worker's expected input size
            padding_size = len(sample) - feature_subset_size
            padded_sample = np.concatenate([reduced_sample, np.zeros(padding_size)])

            # Add noise to simulate reduced accuracy
            noise = np.random.normal(0, 0.01, padded_sample.shape)
            worker_sample = padded_sample + noise
        else:  # Full mode - use complete feature set
            worker_sample = sample

        # Create worker observation (simplified version of WorkerOnlyEnvironment observation)
        basic_stats = np.array([
            self.current_step / self.episode_length,
            self.episode_metrics['total_reward'] / max(1, self.current_step),
            mode,  # Detection mode as feature
            0, 0, 0, 0, 0, 0, 0  # Placeholder stats
        ], dtype=np.float32)

        worker_obs = np.concatenate([worker_sample, basic_stats])
        return worker_obs

    def _calculate_hierarchical_reward(self, manager_action: int, worker_action: int, true_label: int) -> float:
        """Calculate reward for hierarchical system"""
        # Base reward from worker action
        base_reward = 0
        if worker_action == 1:  # Block
            if true_label == 1:  # Correctly blocked malicious
                base_reward = 10.0
            else:  # Incorrectly blocked benign
                base_reward = -2.0
        elif worker_action == 0:  # Allow
            if true_label == 0:  # Correctly allowed benign
                base_reward = 1.0
            else:  # Incorrectly allowed malicious
                base_reward = -20.0
        else:  # Log
            base_reward = 0.5

        # Cost penalty based on detection mode
        mode_cost = self.cost_lightweight if manager_action == 0 else self.cost_full

        # Total reward = base reward - mode cost
        total_reward = base_reward - mode_cost

        return total_reward

class EnvironmentFactory:
    """Factory class for creating IDS environments"""

    @staticmethod
    def create_worker_environment(X_train: np.ndarray, y_train: np.ndarray, max_episodes: int = 1000) -> WorkerOnlyEnvironment:
        """Create worker-only environment for initial training"""
        return WorkerOnlyEnvironment(X_train, y_train, max_episodes)

    @staticmethod
    def create_hierarchical_environment(X_train: np.ndarray, y_train: np.ndarray, worker_agent, max_episodes: int = 1000) -> HierarchicalIDSEnvironment:
        """Create hierarchical environment with trained worker agent"""
        return HierarchicalIDSEnvironment(X_train, y_train, worker_agent, max_episodes)

def evaluate_environment(env: BaseIDSEnvironment, agent, num_episodes: int = 10) -> Dict:
    """
    Evaluate agent performance in environment

    Args:
        env: Environment to evaluate in
        agent: Trained agent
        num_episodes: Number of episodes to evaluate

    Returns:
        Dictionary of evaluation metrics
    """
    total_metrics = {
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f1_score': [],
        'total_reward': [],
        'episode_length': []
    }

    for episode in range(num_episodes):
        obs, info = env.reset()
        episode_reward = 0
        episode_metrics = {
            'true_positives': 0,
            'false_positives': 0,
            'true_negatives': 0,
            'false_negatives': 0
        }

        terminated = False
        step_count = 0

        while not terminated and step_count < 1000:  # Safety limit
            action, _ = agent.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)

            episode_reward += reward
            step_count += 1

            # Update episode metrics from environment
            if 'metrics' in info:
                episode_metrics = info['metrics'].copy()

        # Calculate metrics
        tp = episode_metrics['true_positives']
        fp = episode_metrics['false_positives']
        tn = episode_metrics['true_negatives']
        fn = episode_metrics['false_negatives']

        # Avoid division by zero
        accuracy = (tp + tn) / max(1, tp + fp + tn + fn)
        precision = tp / max(1, tp + fp)
        recall = tp / max(1, tp + fn)
        f1_score = 2 * (precision * recall) / max(1, precision + recall)

        total_metrics['accuracy'].append(accuracy)
        total_metrics['precision'].append(precision)
        total_metrics['recall'].append(recall)
        total_metrics['f1_score'].append(f1_score)
        total_metrics['total_reward'].append(episode_reward)
        total_metrics['episode_length'].append(step_count)

    # Calculate averages
    avg_metrics = {}
    for metric, values in total_metrics.items():
        avg_metrics[f'avg_{metric}'] = np.mean(values)
        avg_metrics[f'std_{metric}'] = np.std(values)

    return avg_metrics

def main():
    """
    Example usage of the environments
    """
    print("IDS Environment Example")
    print("=" * 30)

    # This would typically load preprocessed data
    # For demonstration, create dummy data
    X_dummy = np.random.randn(1000, 20).astype(np.float32)
    y_dummy = np.random.randint(0, 2, 1000).astype(np.int32)

    print("Creating WorkerOnlyEnvironment...")
    worker_env = EnvironmentFactory.create_worker_environment(X_dummy, y_dummy, max_episodes=100)

    print(f"Worker Environment:")
    print(f"- Action space: {worker_env.action_space}")
    print(f"- Observation space: {worker_env.observation_space}")
    print(f"- Data size: {len(X_dummy)}")

    # Reset environment
    obs, info = worker_env.reset()
    print(f"- Initial observation shape: {obs.shape}")
    print(f"- Initial info: {info}")

    # Take a random action
    action = worker_env.action_space.sample()
    obs, reward, terminated, truncated, info = worker_env.step(action)
    print(f"- After action {action}: reward={reward:.2f}, terminated={terminated}")

    print("\nEnvironment setup completed successfully!")

if __name__ == "__main__":
    main()